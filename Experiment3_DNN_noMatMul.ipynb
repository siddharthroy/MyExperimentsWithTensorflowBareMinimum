{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DNN with explicit weight elements (no matmul usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense NN:\n",
    " \n",
    "Two layers.\n",
    "\n",
    "Two inputs  ---------->  layer1:  3-nodes   ------------> layer2:  2-output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()      # Whenever you restart and want to clean your graph board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and outputs\n",
    "x1 = tf.placeholder(tf.float32, name=\"x1\")\n",
    "x2 = tf.placeholder(tf.float32, name=\"x2\")\n",
    "\n",
    "y1 = tf.placeholder(tf.float32, name=\"y1\")\n",
    "y2 = tf.placeholder(tf.float32, name=\"y2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAG for DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st layer with 3 nodes (every node is an add op followed by relu activation)\n",
    "\n",
    "# All weights & biases first (my convention: w123 mean connection from 2->3, for layer 1)\n",
    "                                        #    w <layer> <feedin_neuron> <feedto_neuron>\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    w111 = tf.Variable(np.random.rand(),name=\"w111\")\n",
    "    w112 = tf.Variable(np.random.rand(),name=\"w112\")\n",
    "    w113 = tf.Variable(np.random.rand(),name=\"w113\")\n",
    "    w121 = tf.Variable(np.random.rand(),name=\"w121\")\n",
    "    w122 = tf.Variable(np.random.rand(),name=\"w122\")\n",
    "    w123 = tf.Variable(np.random.rand(),name=\"w123\")\n",
    "    \n",
    "    #bias convention: b12 means b<row><col> : <col> acts like layer\n",
    "    b11 = tf.Variable(np.random.rand(),name=\"b11\")\n",
    "    b21 = tf.Variable(np.random.rand(),name=\"b21\")\n",
    "    b31 = tf.Variable(np.random.rand(),name=\"b31\")\n",
    "    \n",
    "    # Node convention n12 means n<row><col> : <col> acts like layer\n",
    "    with tf.name_scope(\"Neuron1\"):\n",
    "        n11 = x1*w111 + x2*w121 + b11\n",
    "        a11 = tf.nn.relu(n11)\n",
    "    with tf.name_scope(\"Neuron2\"):\n",
    "        n21 = x1*w112 + x2*w122 + b21\n",
    "        a21 = tf.nn.relu(n21)\n",
    "    with tf.name_scope(\"Neuron3\"):\n",
    "        n31 = x1*w113 + x2*w123 + b31\n",
    "        a31 = tf.nn.relu(n31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd layer with 2 nodes\n",
    "\n",
    "# All weights & biases first \n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    w211 = tf.Variable(np.random.rand(),name=\"w211\")\n",
    "    w212 = tf.Variable(np.random.rand(),name=\"w212\")\n",
    "    w221 = tf.Variable(np.random.rand(),name=\"w221\")\n",
    "    w222 = tf.Variable(np.random.rand(),name=\"w222\")\n",
    "    w231 = tf.Variable(np.random.rand(),name=\"w231\")\n",
    "    w232 = tf.Variable(np.random.rand(),name=\"w222\")\n",
    "    \n",
    "    #bias convention: b12 means n<row><col> : <col> acts like layer\n",
    "    b12 = tf.Variable(np.random.rand(),name=\"b12\")\n",
    "    b22 = tf.Variable(np.random.rand(),name=\"b22\")\n",
    "    \n",
    "    # Node convention n12 means n<row><col> : <col> acts like layer\n",
    "    with tf.name_scope(\"Neuron1\"):\n",
    "        n12 = a11*w211 + a21*w221 + a31*w231 + b12\n",
    "        a12 = tf.nn.relu(n12)    \n",
    "    with tf.name_scope(\"Neuron2\"):\n",
    "        n22 = a11*w212 + a21*w222 + a31*w232 + b22\n",
    "        a22 = tf.nn.relu(n22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.sqrt((y1 - a12)**2 + (y2 - a22)**2, name=\"loss\")\n",
    "cost = tf.reduce_mean(loss, name=\"tot_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer:\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "set_to_optimize = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the graph so far\n",
    "writer = tf.summary.FileWriter(logdir=\"./graph\",graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare input and output batch (trivial example)\n",
    "np.random.seed(111)\n",
    "x1_arr = np.random.randint(100, size=1000)\n",
    "x2_arr = np.random.randint(100, size=1000)\n",
    "y1_arr = np.random.randint(100, size=1000)\n",
    "y2_arr = np.random.randint(100, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture summary to plot\n",
    "summary_cost = tf.summary.scalar(\"cost\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the weights 1000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Trivial case of training: Using full batch to iterate everytime (no mini batching)\n",
    "for i in range(1000):     \n",
    "    sess.run(set_to_optimize, feed_dict={x1:x1_arr, x2:x2_arr, y1:y1_arr, y2:y2_arr})\n",
    "    s_buffer = sess.run(summary_cost, feed_dict={x1:x1_arr, x2:x2_arr, y1:y1_arr, y2:y2_arr})\n",
    "    writer.add_summary(s_buffer,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.25396"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(cost, feed_dict={x1:x1_arr, x2:x2_arr, y1:y1_arr, y2:y2_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
